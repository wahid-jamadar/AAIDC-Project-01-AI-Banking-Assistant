# **RAG-Based AI-Banking Assistant (AAIDC Project 1)**

ğŸš€ *A fully implemented Retrieval-Augmented Generation system using ChromaDB, HuggingFace embeddings, and multiple LLM providers.*

---

## ğŸ¤– **What is this?**

This project is a fully working **RAG (Retrieval-Augmented Generation)** AI assistant. It allows you to ask questions based on your own documents by combining:

* ğŸ“„ Document loading
* ğŸ” Vector similarity search
* ğŸ§  Embedding-based retrieval
* ğŸ’¬ LLM powered answer generation

Think of it as:

> **"ChatGPT that knows your documents."**

---

## ğŸ¯ **What This Project Can Do**

Your RAG assistant can:

* ğŸ“‚ Load `.txt` documents from the `data/` directory
* âœ‚ï¸ Automatically chunk documents
* ğŸ”¢ Generate embeddings using SentenceTransformers
* ğŸ§¬ Store vectors in a persistent ChromaDB database
* ğŸ” Retrieve relevant chunks using similarity search
* ğŸ’¬ Generate accurate answers using OpenAI, Groq, or Google Gemini
* ğŸ”§ Automatically choose the available LLM provider

---

## ğŸ§  **How It Works**

1. **Document Loading:** Reads `.txt` files from `data/`.
2. **Chunking:** Splits large documents using LangChainâ€™s `RecursiveCharacterTextSplitter`.
3. **Embeddings:** Converts text into vectors using `all-MiniLM-L6-v2`.
4. **Vector DB:** Stores embeddings in ChromaDB with persistent local storage.
5. **Similarity Search:** Retrieves top-k relevant chunks.
6. **Prompting:** A custom RAG prompt passes context + question to the LLM.
7. **LLM Answer:** OpenAI â†’ Groq â†’ Gemini (auto fallback).

---

## ğŸ§© **Features**

âœ” Fully implemented RAG pipeline <br>
âœ” Multi-provider LLM support <br>
âœ” ChromaDB persistent storage <br>
âœ” Automatic document chunking <br>
âœ” Uses HuggingFace embeddings <br>
âœ” Clean modular structure <br>
âœ” No manual setup inside code <br>
âœ” Works with any `.txt` data 

---

## ğŸ“¦ **Project Structure**

```
Module_01_Project_01/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py            # Main RAG engine
â”‚   â””â”€â”€ vectordb.py       # Chroma + Embeddings + Chunking + Search
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ banking_data01.txt
â”‚   â”œâ”€â”€ banking_data02.txt
â”œâ”€â”€ chroma_db/            # Auto-created persistent DB folder
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## âš™ï¸ **Setup Instructions**

### **1ï¸âƒ£ Install Dependencies**

```bash
pip install -r requirements.txt
```

---

### **2ï¸âƒ£ Configure Your API Key**

Add your API key to `.env`:

```
OPENAI_API_KEY=your_key_here
# OR
GROQ_API_KEY=your_key_here
# OR
GOOGLE_API_KEY=your_key_here
```

> The system will automatically select the first available one.

---

### **3ï¸âƒ£ Add Your Documents**

Place `.txt` files into the `data/` folder.

Example:

```
data/
â”œâ”€â”€ banking_data01.txt
â”œâ”€â”€ banking_data02.txt
```

---

### **4ï¸âƒ£ Run the RAG Assistant**

```bash
python src/app.py
```

Example interaction:

```
Your question: What is KYC in banking?

ANSWER:
KYC (Know Your Customer) is a process...
```

---

## ğŸ§ª **Testing the Components**

### **Test Chunking**

```python
from src.vectordb import VectorDB
v = VectorDB()
print(v.chunk_text("Sample document test"))
```

### **Test Vector Search**

```python
v.search("banking")
```

### **Test Full RAG**

Run:

```bash
python src/app.py
```

Ask:

```
Explain the NEFT payment system.
```

---

## ğŸ› ï¸ **Tech Stack**

* **Python**
* **ChromaDB** (Vector database)
* **SentenceTransformers** (Embeddings)
* **LangChain** (Chunking + Prompting)
* **LLM Providers:**

  * OpenAI
  * Groq (Used In My Project)
  * Google Gemini

---

## ğŸ§‘â€ğŸ’» **Author**

**Wahid Jamdar** <br>
B.Tech CSE <br>
DY Patil Agriculture & Technical University, Kolhapur <br>

---

## ğŸ“„ **License**

This project is created as part of **AAIDC Project-1** and intended for educational use.

---
